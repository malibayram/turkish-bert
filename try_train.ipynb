{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Total samples: 2001\n",
      "Training samples: 1800\n",
      "Testing samples: 201\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from bert.dataset import BERTDataset, collate_batch\n",
    "from turkish_tokenizer.turkish_tokenizer import TurkishTokenizer\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    num_workers = 4\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    num_workers = 0  # Use 0 workers with MPS\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    num_workers = 4\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = TurkishTokenizer()\n",
    "\n",
    "# Create dataset with device\n",
    "dataset = BERTDataset(\n",
    "    corpus_path=\"combined_reviews.csv\",  # Changed from beyazperde_yorumlar.csv\n",
    "    tokenizer=tokenizer,\n",
    "    seq_len=512,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create train/test split\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create dataloaders with custom collate function\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Total Parameters: 135404290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bert.trainer.BERTTrainer at 0x1108b8880>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert import BERT, BERTLM, BERTTrainer\n",
    "\n",
    "# Model hyperparameters\n",
    "vocab_size = 32768\n",
    "d_model = 768\n",
    "n_layers = 12\n",
    "heads = 12\n",
    "dropout = 0.1\n",
    "seq_length = 512\n",
    "\n",
    "# Create models\n",
    "bert = BERT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    "    seq_len=seq_length\n",
    ")\n",
    "\n",
    "bert_lm = BERTLM(\n",
    "    bert=bert,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = BERTTrainer(\n",
    "    model=bert_lm,\n",
    "    train_dataloader=train_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    lr=1e-4,\n",
    "    warmup_steps=10000,\n",
    "    device=device  # It will automatically use MPS if available\n",
    ")\n",
    "\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   2%|| 1/57 [00:10<09:28, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 1.927762746810913, 'avg_acc': 53.125, 'loss': 1.927762746810913}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  19%|| 11/57 [00:46<01:59,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 10, 'avg_loss': 1.8806931539015337, 'avg_acc': 47.72727272727273, 'loss': 1.831728458404541}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  37%|| 21/57 [01:09<01:21,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 20, 'avg_loss': 1.763727653594244, 'avg_acc': 48.80952380952381, 'loss': 1.61106276512146}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  54%|| 31/57 [01:32<00:59,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 30, 'avg_loss': 1.6815672382231681, 'avg_acc': 49.395161290322584, 'loss': 1.508908748626709}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  72%|| 41/57 [01:54<00:36,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 40, 'avg_loss': 1.6300582420535203, 'avg_acc': 49.84756097560975, 'loss': 1.5275278091430664}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:  89%|| 51/57 [02:18<00:13,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 50, 'avg_loss': 1.5857696626700608, 'avg_acc': 50.0, 'loss': 1.3152116537094116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0: 100%|| 57/57 [02:30<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0, train:             avg_loss=1.5580961871565433,             total_acc=50.27777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0:  14%|| 1/7 [00:01<00:07,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 1.3927658796310425, 'avg_acc': 50.0, 'loss': 1.3927658796310425}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_test:0: 100%|| 7/7 [03:08<00:00, 26.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP0, test:             avg_loss=1.3189232349395752,             total_acc=45.27363184079602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    trainer.train(epoch)\n",
    "    trainer.test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
